# Learning to Summarize: Training

**Paper**: ["Learning to summarize from human feedback"](https://arxiv.org/abs/2009.01325) (Stiennon et al., September 2020, NeurIPS 2020)

The first application of RLHF to a GPT-3 class model. Fine-tuned GPT-3 (1.3B and 6.7B) to summarize Reddit posts using the pipeline that would become the industry standard: supervised fine-tuning, reward modeling, and PPO. Published just 3 months after GPT-3, this was the proof of concept that RLHF works on large language models.

## The Problem: ROUGE Is Broken

The standard metric for evaluating summaries was ROUGE — a family of metrics that measure n-gram overlap between a generated summary and a reference summary. The paper's central motivation is that **optimizing ROUGE actually hurts summary quality** as judged by humans.

This is Goodhart's Law applied to summarization: "when a measure becomes a target, it ceases to be a good measure." ROUGE captures surface-level similarity (word overlap) but not what makes a summary genuinely good (accuracy, coverage, coherence). A model optimizing for ROUGE learns to game word overlap rather than produce better summaries.

The paper demonstrates this directly — models that score higher on ROUGE produce summaries that humans rate as worse. The solution: replace the automated metric with a learned reward model trained on human preferences, then optimize against that.

## Data

### Task Dataset

**TL;DR Reddit dataset** — posts from Reddit where authors included a "TL;DR" (too long; didn't read) summary. The dataset was filtered from 3 million posts down to ~123,000 high-quality posts with summaries between 24-48 tokens.

### Human Comparisons (~64,000)

For the same Reddit post, two model-generated summaries were shown side by side. Human labelers picked which summary was better. These pairwise comparisons trained the reward model.

- ~64,000 total comparisons
- Collected from summaries generated by models at various stages of training
- Labelers judged overall quality (accuracy, coverage, coherence)

## Model Sizes

Two sizes from the GPT-3 family: **1.3B** and **6.7B** parameters. Not the full 175B — this was a focused experiment on a single task, not yet scaled to the flagship model.

## The Pipeline: SFT → RM → PPO

The minimal, original version of the RLHF pipeline. Each stage builds on the previous one.

### Stage 1: Supervised Fine-Tuning (SFT)

Fine-tune GPT-3 on the TL;DR Reddit summaries using standard next-token prediction. The model learns the format and basic competence at summarization.

This is the starting point — a model that can summarize but is limited to imitating the reference summaries in the dataset.

### Stage 2: Reward Model (RM)

A model that scores summary quality, trained on human pairwise comparisons.

**Architecture**: Same transformer as the policy, with the language model head replaced by a single linear layer that outputs a scalar score. Critically, the RM is the **same size as the policy** — a 6.7B policy uses a 6.7B reward model.

**Training**: Uses the **Bradley-Terry model** on pairwise comparisons:

Given a post P with two summaries A and B where a human preferred A:

1. Forward pass: feed (P + A) → get score_A
2. Forward pass: feed (P + B) → get score_B
3. Loss: `-log(sigmoid(score_A - score_B))`

The sigmoid converts the score difference into a preference probability. The loss pushes the preferred summary's score higher.

**Data**: ~64,000 pairwise human comparisons.

### Stage 3: Reinforcement Learning (PPO)

Fine-tunes the SFT model using [[llm/training-techniques/ppo|PPO]] with the reward model providing the reward signal.

1. The SFT model (now the "policy") generates a summary for a Reddit post
2. The reward model scores the summary
3. PPO updates the policy to produce higher-scoring summaries
4. A **KL divergence penalty** constrains the policy to stay close to the SFT model

The KL penalty is essential — without it, the model drifts too far from coherent language and learns to exploit flaws in the reward model rather than producing genuinely better summaries. This is the first mitigation for what the paper identifies as reward overoptimization (see [[llm/models/gpt3/gpt3-to-chatgpt/summarization/evaluation|evaluation]]).

## Comparison: The Minimal Pipeline vs Later Work

This paper established the minimal version of the RLHF pipeline. WebGPT and InstructGPT built on it with additions:

| Aspect | Summarization (2020) | WebGPT (2021) | InstructGPT (2022) |
|---|---|---|---|
| **Task** | One task (summarization) | One task (QA with browsing) | All tasks (general instructions) |
| **Comparison data** | Pairwise (A vs B) | Pairwise (A vs B) | Rankings (rank 4-9 outputs) |
| **RM size** | Same size as policy | Same size as policy | Smaller than policy (6B RM for 175B policy) |
| **Rejection sampling** | No | Yes (best-of-n) | Yes (best-of-n) |
| **PPO-ptx** | No | No | Yes (mix in pre-training gradients) |
| **Model sizes** | 1.3B, 6.7B | 760M, 13B, 175B | 1.3B, 6B, 175B |

Key differences:

- **Pairwise vs rankings**: This paper collected simple A-vs-B comparisons. InstructGPT had labelers rank 4-9 outputs, extracting more comparison signal per labeling session.
- **Same-size RM**: Both this paper and WebGPT use RMs the same size as the policy. InstructGPT decoupled them — using a 6B RM to train a 175B policy — which was more compute-efficient.
- **No rejection sampling**: This paper used only PPO. WebGPT showed rejection sampling could match or beat RL. InstructGPT used both.
- **No PPO-ptx**: InstructGPT added pre-training gradient mixing during PPO to prevent catastrophic forgetting on general tasks. Not needed here since the model only had to do one task.

## Significance

This paper proved three things:

1. **RLHF works on LLMs** — the first demonstration at GPT-3 scale
2. **Human feedback > automated metrics** — optimizing human preferences produces better results than optimizing ROUGE
3. **The SFT → RM → PPO pipeline works** — this exact pipeline, with refinements, powered InstructGPT and ChatGPT

Key authors (Long Ouyang, Jeff Wu) went on to lead InstructGPT, carrying this pipeline from a single-task proof of concept to general instruction following.

---

## Related Documents

- [[llm/models/gpt3/gpt3-to-chatgpt/summarization/evaluation|Summarization Evaluation]] - Results, reward hacking, and analysis
- [[llm/models/gpt3/gpt3-to-chatgpt/overview|GPT-3 to ChatGPT Overview]] - Timeline and evolution
- [[llm/models/gpt3/gpt3-to-chatgpt/webgpt/training|WebGPT Training]] - Next RLHF application (QA with browsing)
- [[llm/models/gpt3/gpt3-to-chatgpt/instructgpt/training|InstructGPT Training]] - Generalized RLHF for all tasks
- [[llm/training-techniques/ppo|PPO for LLMs]] - The RL algorithm used
